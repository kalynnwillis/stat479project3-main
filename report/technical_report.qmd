---
title: "Technical Report: Receiver Movement during Ball Flight"
author: "Eleanor Brothers, Kalynn Willis, Jasmine Peck"
date: 12/02/2025
format:
  html:
    code-fold: true
    code-summary: "Show code"
execute:
  echo: true
  warning: false
  message: false
---

## Executive Summary

During a Week 7 Saints-Bears game in the 2025 regular season, the New Orleans Saints wide receiver Chris Olave caught a 57-yard completion to advance his team to the 20-yard line. A replay of this play can be viewed here: [Chris Olave 57-yard catch vs Bears (Week 7, 2025)](https://www.youtube.com/watch?v=Fxqq43Bfyf4). Throughout the play, Olave gains yards on his defender after the ball is thrown, giving him more space to catch it. NFL tracking data tells where the ball is at the throw, catch, or incomplete, but does not reveal what happened between the receiver and defender while the ball was in flight. 

Using the 2023 regular season tracking data from the Big Data Bowl (Weeks 1 - 18), we analyze the historical receiver-defender separation for every targeted receiver by creating a Ball Flight Differential (BFD) metric. BFD measures differences in separation between the receiver and their nearest defender during the throw. Then, we group separation into four coverage states (“tight”, “moderate”, “open”, and “wide-open”) and measure how often each receiver transitions between states every tenth of a second. This allows us to identify which receivers consistently create separation during ball flight beyond what is expected based on game context. Using information available at the time of the throw, we also predict how much separation will exist at the end of the play.

Our model and analysis found that league-wide, receivers usually lose separation while the ball is in the air, with the average BFD being −0.82 yards. This means defenders often close the gap on the targeted receiver, especially in certain long throws when the receiver is “waiting” for the ball to catch up to them. Still, some receivers are able to consistently generate a positive BFD Over Expected, which signals to coaches that they are good at creating separation from their defender, even after accounting for route, coverage, etc. For example, Olave has a small, positive BFD Over Expected, where he creates slightly more separation than expected and does so quite consistently. On the other hand, Kalif Raymond has a slightly larger positive BFD Over Expected, where he may create more separation on average, but has less consistency due to scheme, play, routes, etc.

We observed that the league-wide probability of receivers opening up from being covered is about 1.5% of the time per tenth of a second. However, top receivers get themselves open more often by beating that within a few extra percentage points. Our predictive models show that late separation is most influenced by a receiver’s initial separation and the ball’s air time.

These metrics support coaches and scouts in determining which receivers created separation based on their skill alone, rather than other contextual factors. Our analysis helps identify the receivers that create openness late in their running route, which is significant when there are critical downs. Coaches can use the predictive model to design plays that maximize separation opportunities between receivers and their defenders.

## Introduction

### Motivation and Problem Statement

In the NFL, how many yards a receiver is away from their defender is typically recorded at either the throw or the catch, but this metric is incomplete – while the ball is in the air, strong receivers may create additional space from the defender or defenders may gain yards on the receiver.

### Football Example

For example, during a week 7 Saints-Bears game in the 2025 regular season, the New Orleans Saints wide receiver Chris Olave caught a 57-yard catch completion to advance his team to the 20-yard line. A replay of this play can be viewed here: [Chris Olave 57-yard catch vs Bears (Week 7, 2025)](https://www.youtube.com/watch?v=Fxqq43Bfyf4).

Throughout the play, Chris Olave appears to gain yards on his defender after the ball was thrown, allowing him more space to catch the ball and secure further yards. This type of example motivates our central question: which receivers consistently create or maintain separation while the ball is in the air?

Determining which receivers can shake their defender is important for their teammates, coaches, scouts, and fans alike. If a quarterback knows a receiver is skilled, they may be able to throw to them in a critical situation and trust that they can generate space from their defender and get open for the throw. Coaches and scouts can use this information to build their team when drafting new wide receivers – if a wide receiver had caught many throws to gain many yards with a previous team, it is imperative to understand if that was their skill alone or due to a good quarterback. Finally, fans can even use this information when drafting their fantasy football team. Receivers who consistently gain separation are more likely to secure catches and generate scoring opportunities, making them reliable fantasy contributors.

The Big Data Bowl tracking data allows us to examine these receiver-defender situations more closely. We convert this raw position data into metrics that analyze how much separation is gained or lost while the ball is in the air and identify receiver skill and their capability to shake off their nearest defender.

### Research Questions
To generate our metrics, we address the following questions:
How many yards do receivers typically gain or lose from their nearest defender throughout the throw time?
When controlling for other factors that influence ball-flight differential (defenders, teams, and situations), who are the best receivers?


## Data Description

### Data Sources

To address our research questions, we use the Big Data Bowl 2026 tracking and play-level data that covers the 2023 NFL regular season (Weeks 1 - 18). The play-level data includes game context (down, distance, time, score), offensive/defensive descriptors, and event-level identifiers like catch or incompletion. In addition, the tracking data contains frame-level positions for all of the players and the ball at 10 Hz. The focus of our project is on tracking moments from the throw to the end of the play, so that we can analyze the receiver-defender interactions when a ball is thrown.

```{r}
library(tidyverse)
library(readr)

if (dir.exists("data")) {
  DATA_DIR <- "data"
} else if (dir.exists("../data")) {
  DATA_DIR <- "../data"
} else {
  warning("Could not find data/ directory.")
  DATA_DIR <- "data"
}

WEEKS <- sprintf("w%02d", 1:18) 

load_week_data <- function(week) {
  input_file <- file.path(DATA_DIR, paste0("input_2023_", week, ".csv"))
  output_file <- file.path(DATA_DIR, paste0("output_2023_", week, ".csv"))

  if (!file.exists(input_file)) {
    warning(paste("Input file not found:", input_file))
    return(NULL)
  }
  if (!file.exists(output_file)) {
    warning(paste("Output file not found:", output_file))
    return(NULL)
  }
  input_data <- read_csv(input_file, show_col_types = FALSE)
  output_data <- read_csv(output_file, show_col_types = FALSE)

  return(list(input = input_data, output = output_data))
}

load_tracking_data <- function(weeks = WEEKS) {

  all_data <- list()

  for (week in weeks) {
    week_data <- load_week_data(week)
    if (!is.null(week_data)) {
      all_data[[week]] <- week_data
    }
  }

  return(all_data)
}

if (sys.nframe() == 0) {
  if (length(list.files(DATA_DIR, pattern = "\\.csv$")) == 0) {
    stop("No CSV files found in data/ directory.")
  }

  data <- load_tracking_data()
}
```

### Sample Construction

To narrow down the data, we filter to only the plays with an identified targeted receiver, throw frame, and play end frame. We remove any plays where tracking may be incomplete during a ball’s flight. Finally, we computed the distance between the identified receiver and the closest defender at each individual frame from the throw to the play end.
Our filtered dataset contains 14,108 passing plays, which each contain the receiver-defender separation at the throw and play end, the air time, and the contextual variables (down, score, routes run, etc.)

```{r}
library(tidyverse)
identify_targeted_receiver <- function(play_data) {
  tr <- play_data |>
    filter(player_role == "Targeted Receiver") |>
    distinct(nfl_id) |>
    pull(nfl_id)

  if (length(tr) == 0) {
    return(NA_integer_)
  } 
  if (length(tr) > 1) {
    warning("Multiple targeted receivers found in one play")
    tr <- tr[1]
  }
  tr
}

calculate_separation <- function(receiver_loc, defenders_loc) {
  if (nrow(receiver_loc) == 0 || nrow(defenders_loc) == 0) {
    return(NA_real_)
  }
  rx <- receiver_loc$x[1]
  ry <- receiver_loc$y[1]
  dx <- defenders_loc$x
  dy <- defenders_loc$y
  min(sqrt((rx - dx)^2 + (ry - dy)^2))
}

compute_play_separation <- function(gid, pid, input_df, output_df) {
  play_in <- input_df |> filter(game_id == gid, play_id == pid)
  play_out <- output_df |> filter(game_id == gid, play_id == pid)

  if (nrow(play_in) == 0 || nrow(play_out) == 0) {
    return(NULL)
  }

  tr_id <- identify_targeted_receiver(play_in)
  if (is.na(tr_id)) {
    return(NULL)
  }

  throw_frame <- max(play_in$frame_id, na.rm = TRUE)
  rec_throw <- play_in |> filter(nfl_id == tr_id, frame_id == throw_frame)

  tr_name_vec <- play_in |>
    filter(nfl_id == tr_id) |>
    distinct(player_name) |>
    pull(player_name)

  tr_name <- if (length(tr_name_vec) > 0) tr_name_vec[1] else NA_character_

  tr_pos_vec <- play_in |>
    filter(nfl_id == tr_id) |>
    distinct(player_position) |>
    pull(player_position)

  tr_pos <- if (length(tr_pos_vec) > 0) tr_pos_vec[1] else NA_character_
  defs_throw <- play_in |>
    filter(player_role == "Defensive coverage", frame_id == throw_frame)

  if (nrow(defs_throw) == 0) {
    message(
      "No 'Defensive coverage' labels for game ", gid,
      ", play ", pid, ", using non-offensive players at throw_frame as defenders."
    )
    defs_throw <- play_in |>
      filter(!player_role %in% c("Targeted Receiver", "Passer", "Other Route Runner"), frame_id == throw_frame)
  }

  defender_ids <- unique(defs_throw$nfl_id)

  d_throw <- calculate_separation(rec_throw, defs_throw)

  #In-air frames
  rec_out <- play_out |> filter(nfl_id == tr_id)
  if (nrow(rec_out) == 0) {
    return(NULL)
  }

  first_out_frame <- min(rec_out$frame_id, na.rm = TRUE)
  catch_frame <- max(rec_out$frame_id, na.rm = TRUE)
  rec_catch <- rec_out |> filter(frame_id == catch_frame)

  defs_catch <- play_out |>
    filter(nfl_id %in% defender_ids, frame_id == catch_frame)

  d_catch <- calculate_separation(rec_catch, defs_catch)

air_time <- (catch_frame - first_out_frame) * 0.1

  #Separation Computed

  frames <- unique(rec_out$frame_id)

  traj_list <- map(frames, function(f) {
    rec_loc <- rec_out |> filter(frame_id == f)
    defs <- play_out |> filter(frame_id == f, nfl_id %in% defender_ids)

    tibble(
      game_id = gid,
      play_id = pid,
      frame_id = f,
      separation = calculate_separation(rec_loc, defs)
    )
  })

  traj_df <- bind_rows(traj_list)

  list(
    play_features = tibble(
      game_id = gid,
      play_id = pid,
      targeted_id = tr_id,
      targeted_name = tr_name,
      targeted_position = tr_pos,
      throw_frame = throw_frame,
      catch_frame = catch_frame,
      d_throw = d_throw,
      d_catch = d_catch,
      air_time = air_time,
      ball_land_x = first(play_in$ball_land_x),
      ball_land_y = first(play_in$ball_land_y)
    ),
    trajectory = traj_df
  )
}

build_play_feature_table <- function(week_data) {
  input_df <- week_data$input
  output_df <- week_data$output

  plays <- input_df |> distinct(game_id, play_id)

  all_features <- list()
  all_trajectories <- list()

  for (i in 1:nrow(plays)) {
    p <- plays[i, ]
    res <- compute_play_separation(p$game_id, p$play_id, input_df, output_df)

    if (!is.null(res)) {
      all_features[[length(all_features) + 1]] <- res$play_features
      all_trajectories[[length(all_trajectories) + 1]] <- res$trajectory
    }

    if (i %% 100 == 0) cat(".")
  }

  features_df <- if (length(all_features) > 0) dplyr::bind_rows(all_features) else tibble()
  traj_df <- if (length(all_trajectories) > 0) dplyr::bind_rows(all_trajectories) else tibble()

  list(
    features = features_df,
    trajectories = traj_df
  )
}

if (sys.nframe() == 0) {
  if (file.exists("src/utils_paths.R")) {
    source("src/utils_paths.R")
  } else if (file.exists("utils_paths.R")) {
    source("utils_paths.R")
  } else {
    stop("Cannot find utils_paths.R")
  }

  if (file.exists("src/00_load_data.R")) {
    source("src/00_load_data.R")
  } else if (file.exists("00_load_data.R")) {
    source("00_load_data.R")
  } else {
    stop("Cannot find 00_load_data.R")
  }

  OUT_DIR <- get_proc_dir()

  for (week in WEEKS) {
    week_data_list <- load_week_data(week)

    if (is.null(week_data_list)) {
      next
    }

    results <- build_play_feature_table(week_data_list)

    feat_file <- file.path(OUT_DIR, paste0("play_features_", week, ".rds"))
    saveRDS(results$features, feat_file)

    traj_file <- file.path(OUT_DIR, paste0("separation_trajectories_", week, ".rds"))
    saveRDS(results$trajectories, traj_file)

    rm(week_data_list, results)
    gc()
  }
}
```

## Methodology

### Ball Flight Differential (BFD)

The play-level metric we have created is an Ball-Flight Differential added (BFD) model:
$$
BFD = d_{catch} - d_{throw},
$$

This measures the difference in distance (in yards) between the targeted receiver and nearest defender at the time of the throw and the ball arrival (catch, pass breakup, or incompletion). Note that a positive BFD indicates the receiver gained separation from their nearest defender, while a negative BFD indicates the defender got closer. 

```{r}
library(tidyverse)

compute_BFD <- function(play_features) {
  play_features |>
    mutate(
      BFD = d_catch - d_throw,
      separation_change_pct = ifelse(d_throw > 0, (d_catch - d_throw) / d_throw, NA_real_),
      is_separation_gained = BFD > 0
    )
}

if (sys.nframe() == 0) {
  if (file.exists("src/utils_paths.R")) {
    source("src/utils_paths.R")
  } else if (file.exists("utils_paths.R")) {
    source("utils_paths.R")
  } else {
    stop("Could not find 'utils_paths.R'.")
  }

  PROC_DIR <- get_proc_dir()

  feature_files <- list.files(PROC_DIR, pattern = "play_features_w[0-9]+\\.rds", full.names = TRUE)

  if (length(feature_files) == 0) {
    stop(paste("No feature files found in", PROC_DIR, ". Run 01_engineer_features.R first."))
  }

  features <- purrr::map_dfr(feature_files, readRDS)
  analysis_df <- compute_BFD(features)
  SUPP_PATH <- NULL
  if (dir.exists("data")) {
    SUPP_PATH <- file.path("data", "BDB2026_supplementary_data.csv")
  } else if (dir.exists("../data")) {
    SUPP_PATH <- file.path("../data", "BDB2026_supplementary_data.csv")
  }

  if (!is.null(SUPP_PATH) && file.exists(SUPP_PATH)) {


    supp <- readr::read_csv(SUPP_PATH, show_col_types = FALSE) |>
      dplyr::select(
        game_id,
        play_id,
        season,
        week,
        play_description,
        offense_formation,
        receiver_alignment,
        route_of_targeted_receiver,
        pass_result,
        pass_length,
        pass_location_type,
        play_action,
        dropback_type,
        dropback_distance,
        defenders_in_the_box,
        team_coverage_man_zone,
        team_coverage_type,
        expected_points,
        expected_points_added,
        yards_gained
      )

    analysis_df <- analysis_df |>
      mutate(
        game_id = as.numeric(game_id),
        play_id = as.numeric(play_id)
      ) |>
      left_join(
        supp |>
          mutate(
            game_id = as.numeric(game_id),
            play_id = as.numeric(play_id)
          ),
        by = c("game_id", "play_id")
      )
  } else {
    message("Supplementary play-level data not found")
  }

  summary_stats <- analysis_df |>
    summarise(
      avg_BFD = mean(BFD, na.rm = TRUE),
      median_BFD = median(BFD, na.rm = TRUE),
      prop_positive = mean(BFD > 0, na.rm = TRUE),
      n_plays = n()
    )

  print(summary_stats)

  saveRDS(analysis_df, file.path(PROC_DIR, "analysis_full.rds"))
}
```

### Separation States and Markov Lift

To take a deeper look at our problem, we utilize the Markov Lift method. We begin by analyzing separation as a dynamic process, using categorical variables to make the distance at each frame distinct: 

- **S0 (Tight)**: \< 1 yard
- **S1 (Moderate)**: 1–3 yards
- **S2 (Open)**: 3–5 yards
- **S3 (Wide Open)**: \> 5 yards

We created a sequence for each play where each frame is assigned a separation category. We then model the probabilities with a first-order Markov chain that estimates the transition probabilities:  
$$
P(S_{t+1} = j \mid S_t = i), \quad i, j \in \{0,1,2,3\}.
$$

In words, we measure the probability of the separation category of the next frame given the previous one. The league-wide transition matrix measures how likely a receiver is to maintain or change separation from frame to frame. The change can either be stagnant, decreasing, or increasing. Next, conceptually grouping S0 and S1 into “covered” states and S2 and S3 into “open” states, we focus on the transition between these two groups. The following formula represents the probability of going from covered to open in a single frame:  
$$
p^{\text{league}}_{\text{break}} = P(S_{t+1} \in \{2,3\} \mid S_t \in \{0,1\})
$$

Per frame, this probability is 1.49%.

Next, we calculate this model’s probability for each receiver based on their frame-level transition. We define Markov Lift as: 
$$
\text{Markov Lift} = p^{\text{player}}_{\text{break}} - p^{\text{league}}_{\text{break}}.
$$

This percentage measures if an individual receiver breaks open from often than the league average (a positive result) or vice versa. 


```{r}
library(tidyverse)

get_state <- function(sep) {
  case_when(
    sep < 1 ~ "S0_Tight",
    sep < 3 ~ "S1_Moderate",
    sep < 5 ~ "S2_Open",
    TRUE ~ "S3_WideOpen"
  )
}

STATE_LEVELS <- c("_Tight", "S1_Moderate", "S2_Open", "S3_WideOpen")

load_trajectories <- function(proc_dir = "processed") {
  files <- list.files(proc_dir, pattern = "separation_trajectories_w[0-9]+\\.rds", full.names = TRUE)
  if (length(files) == 0) stop("No trajectory files found.")

  map(files, readRDS) |>
    dplyr::bind_rows()
}

compute_transitions <- function(traj_df) {
  traj_df |>
    arrange(game_id, play_id, frame_id) |>
    group_by(game_id, play_id) |>
    mutate(
      state = get_state(separation),
      next_state = lead(state)
    ) |>
    filter(!is.na(state), !is.na(next_state)) |>
    ungroup()
}

estimate_transition_matrix <- function(transitions) {
  transitions <- transitions |>
    mutate(
      state = factor(state, levels = STATE_LEVELS),
      next_state = factor(next_state, levels = STATE_LEVELS)
    )

  counts <- transitions |>
    count(state, next_state, .drop = FALSE)

  counts |>
    group_by(state) |>
    mutate(prob = ifelse(sum(n) > 0, n / sum(n), 0)) |>
    ungroup()
}

plot_transition_matrix <- function(trans_probs) {
  ggplot(trans_probs, aes(x = next_state, y = fct_rev(state), fill = prob)) +
    geom_tile() +
    geom_text(aes(label = round(prob, 2)), color = "white") +
    scale_fill_gradient(low = "navy", high = "red") +
    theme_minimal() +
    labs(
      title = "Separation State Transition Probabilities",
      subtitle = "Probability of moving from State Y to State X in next frame",
      x = "Next State",
      y = "Current State",
      fill = "Probability"
    )
}

compute_markov_lift <- function(transitions, features_df) {

  relevant_starts <- c("_Tight", "S1_Moderate")
  good_ends <- c("S2_Open", "S3_WideOpen")
  league_rate <- transitions |>
    filter(state %in% relevant_starts) |>
    summarise(
      success_rate = mean(next_state %in% good_ends)
    ) |>
    pull(success_rate)


  player_stats <- transitions |>
    inner_join(features_df, by = c("game_id", "play_id")) |>
    filter(state %in% relevant_starts) |>
    group_by(targeted_id) |>
    summarise(
      n_transitions = n(),
      successes = sum(next_state %in% good_ends),
      player_rate = successes / n_transitions
    ) |>
    filter(n_transitions >= 50) |>
    mutate(
      markov_lift = player_rate - league_rate
    ) |>
    arrange(desc(markov_lift))

  player_stats
}

compute_break_counts <- function(transitions, features_df, min_transitions = 50) {
  relevant_starts <- c("_Tight", "S1_Moderate")
  good_ends <- c("S2_Open", "S3_WideOpen")

  league_rate <- transitions |>
    filter(state %in% relevant_starts) |>
    summarise(
      success_rate = mean(next_state %in% good_ends)
    ) |>
    pull(success_rate)

  player_counts <- transitions |>
    inner_join(features_df, by = c("game_id", "play_id")) |>
    filter(state %in% relevant_starts) |>
    group_by(targeted_id) |>
    summarise(
      n_transitions = n(),
      successes = sum(next_state %in% good_ends),
      .groups = "drop"
    ) |>
    filter(n_transitions >= min_transitions)

  list(
    break_df = player_counts,
    league_rate = league_rate
  )
}

fit_bayesian_markov_model <- function(break_df) {
  if (!requireNamespace("brms", quietly = TRUE)) {
    warning("Package 'brms' not installed")
    return(NULL)
  }

  break_df <- break_df |>
    mutate(targeted_id = as.factor(targeted_id))

  bayes_fit <- brms::brm(
    successes | trials(n_transitions) ~ 1 + (1 | targeted_id),
    data = break_df,
    family = binomial(),
    prior = c(
      brms::prior(normal(0, 2), class = "Intercept"),
      brms::prior(exponential(1), class = "sd")
    ),
    iter = 2000,
    warmup = 1000,
    chains = 4,
    cores = min(4, parallel::detectCores()),
    control = list(adapt_delta = 0.95),
    refresh = 0
  )

  bayes_fit
}

extract_bayesian_markov_lift <- function(bayes_model, break_df, league_rate) {
  if (is.null(bayes_model) || nrow(break_df) == 0) {
    return(NULL)
  }

  post_prob <- brms::posterior_epred(
    bayes_model,
    newdata = break_df,
    re_formula = ~ (1 | targeted_id)
  )

  summaries <- t(apply(post_prob, 2, function(draws) {
    c(
      player_rate_mean  = mean(draws),
      player_rate_sd    = sd(draws),
      player_rate_lower = quantile(draws, 0.025),
      player_rate_upper = quantile(draws, 0.975),
      markov_lift_mean  = mean(draws) - league_rate,
      markov_lift_lower = quantile(draws - league_rate, 0.025),
      markov_lift_upper = quantile(draws - league_rate, 0.975)
    )
  }))

  summaries <- as_tibble(summaries)

  tibble(
    targeted_id = as.character(break_df$targeted_id),
    n_transitions = break_df$n_transitions,
    successes = break_df$successes
  ) |>
    bind_cols(summaries) |>
    arrange(desc(markov_lift_mean))
}

if (sys.nframe() == 0) {
  if (file.exists("src/utils_paths.R")) {
    source("src/utils_paths.R")
  } else if (file.exists("utils_paths.R")) {
    source("utils_paths.R")
  } else {
    stop("Could not find 'utils_paths.R'.")
  }

  PROC_DIR <- get_proc_dir()

  traj <- load_trajectories(PROC_DIR)

  raw_features <- readRDS(file.path(PROC_DIR, "analysis_full.rds"))

  if (!all(c("pass_length", "targeted_position") %in% names(raw_features))) {
    warning("pass_length or targeted_position missing in analysis_full.rds, using all plays for Markov transitions and Lift.")

    features_df <- raw_features |>
      select(game_id, play_id, targeted_id)

    trans <- compute_transitions(traj)
  } else {
    features_df <- raw_features |>
      filter(
        !is.na(pass_length),
        pass_length > 0,
        targeted_position %in% c("WR", "TE")
      ) |>
      select(game_id, play_id, targeted_id, targeted_position, pass_length)

    valid_plays <- features_df |>
      distinct(game_id, play_id)

    trans <- compute_transitions(traj) |>
      inner_join(valid_plays, by = c("game_id", "play_id"))
  }

  tm <- estimate_transition_matrix(trans)
  print(tm)

  p <- plot_transition_matrix(tm)
  FIG_DIR <- get_fig_dir()
  ggsave(file.path(FIG_DIR, "markov_transitions.png"), p, width = 8, height = 6)

  lift <- compute_markov_lift(trans, features_df)

  df_names <- raw_features
  if ("targeted_name" %in% names(df_names)) {
    name_map <- df_names |>
      distinct(targeted_id, targeted_name) |>
      mutate(targeted_id = as.character(targeted_id))

    lift <- lift |>
      mutate(targeted_id = as.character(targeted_id)) |>
      left_join(name_map, by = "targeted_id") |>
      relocate(targeted_name, .before = targeted_id)
  }

  print(head(lift, 10))

  write_csv(lift, file.path(PROC_DIR, "markov_lift_rankings.csv"))

  break_info <- compute_break_counts(trans, features_df, min_transitions = 50)
  bayes_markov <- fit_bayesian_markov_model(break_info$break_df)
  bayes_lift <- extract_bayesian_markov_lift(bayes_markov, break_info$break_df, break_info$league_rate)

  if (!is.null(bayes_markov) && !is.null(bayes_lift)) {
    if ("targeted_name" %in% names(df_names)) {
      name_map <- df_names |>
        distinct(targeted_id, targeted_name) |>
        mutate(targeted_id = as.character(targeted_id))

      bayes_lift <- bayes_lift |>
        mutate(targeted_id = as.character(targeted_id)) |>
        left_join(name_map, by = "targeted_id") |>
        relocate(targeted_name, .before = targeted_id)
    }

    print(head(bayes_lift, 10))

    saveRDS(bayes_markov, file.path(PROC_DIR, "markov_lift_bayes_model.rds"))
    write_csv(bayes_lift, file.path(PROC_DIR, "markov_lift_rankings_bayes.csv"))
  } else {
    message("Bayesian Markov Lift model not fit, skipping Bayesian lift export.")
  }
}
```

### Mixed‑Effects Model for BFD over Expected

An evident limitation of the BFD model is that it includes many uncontrolled context variables, such as longer air-time throws or certain running routes.

So, we fit a linear mixed-effects BFD model to control for these variables:

$$
BFD_{\text{play}} = \beta_0
  + \beta_1 \cdot \text{Air Time}
  + \beta_2 \cdot d_{throw}
  + \mathbf{x}_{\text{play}}^\top \boldsymbol{\beta}
  + u_{\text{receiver}}
  + u_{\text{game}}
  + \epsilon,
$$

To explain this model, we measure the air time and distance at throw coefficients. We also control for the contextual fixed effects (like route, box count, down, etc.), a receiver-level random effect capturing their separation skill after controlling for context, and a game-level random effect capturing shared conditions (like weather, opponents, etc.). 

We measure BFD over expected, which is defined as the estimated receiver random effect. So if this effect is positive, receivers are creating separation from their defender, and if it’s negative, their defender is getting closer.

To make it more interpretable, we restrict our modeling to forward passes only and have wide receivers and tight ends as a fixed effect. 

We also know there will be uncertainty in our estimates, so we fit a Bayesian analogue of the mixed-effects model. Our priors are used on the fixed effects and on the random-effect standard deviations. This generates a posterior distribution for each receiver’s BFD over expected. Receivers whose BFD over expected was modeled on fewer catches will have wider intervals, reflecting greater uncertainty in our estimate of their Ball-Flight Differential skill. 

Model diagnostics (fitted vs. observed BFD and residual distributions) are used to assess linearity, variance assumptions, and the contribution of random effects.

```{r}
library(tidyverse)
library(lme4)
library(broom.mixed) 

fit_mixed_effects_model <- function(data) {
  m <- lmer(
    BFD ~ air_time_scaled + d_throw_scaled + pass_length_scaled + defenders_in_the_box_scaled +
      team_coverage_man_zone + route_of_targeted_receiver + targeted_position +
      (1 | targeted_id) + (1 | game_id),
    data = data,
    REML = FALSE
  )
  return(m)
}

extract_receiver_rankings <- function(model) {
  ranefs <- ranef(model)$targeted_id

  ranefs |>
    as_tibble(rownames = "nfl_id") |>
    rename(BFD_over_expected = `(Intercept)`) |>
    arrange(desc(BFD_over_expected))
}

fit_bayesian_BFD_model <- function(data) {
  if (!requireNamespace("brms", quietly = TRUE)) {
    warning("Package 'brms' not installed")
    return(NULL)
  }

  bayes_fit <- brms::brm(
    BFD ~ air_time + d_throw + pass_length + defenders_in_the_box +
      team_coverage_man_zone + route_of_targeted_receiver + targeted_position +
      (1 | targeted_id) + (1 | game_id),
    data = data,
    family = gaussian(),
    prior = c(
      brms::prior(normal(0, 2), class = "Intercept"),
      brms::prior(normal(0, 1), class = "b"),
      brms::prior(exponential(1), class = "sd"),
      brms::prior(exponential(1), class = "sigma")
    ),
    iter = 4000,
    warmup = 1000,
    chains = 4,
    cores = min(4, parallel::detectCores()),
    control = list(adapt_delta = 0.95),
    refresh = 0
  )

  bayes_fit
}

extract_bayesian_receiver_rankings <- function(bayes_model) {
  if (is.null(bayes_model)) {
    return(NULL)
  }
  if (!requireNamespace("posterior", quietly = TRUE)) {
    warning("Package 'posterior' not available, skipping Bayesian receiver rankings.")
    return(NULL)
  }

  draws <- brms::as_draws_df(bayes_model)

  if (!"targeted_id" %in% names(bayes_model$data)) {
    warning("Model data does not contain 'targeted_id', skipping Bayesian receiver rankings.")
    return(NULL)
  }

  receiver_ids <- unique(as.character(bayes_model$data$targeted_id))

  summary_list <- purrr::map(receiver_ids, function(rid) {
    cand_cols <- c(
      paste0("r_targeted_id[", rid, ",Intercept]"),
      paste0("r_targeted_id[", rid, ",(Intercept)]")
    )
    col_name <- cand_cols[cand_cols %in% names(draws)][1]
    if (is.na(col_name)) {
      return(NULL)
    }
    vals <- draws[[col_name]]
    tibble(
      nfl_id = rid,
      BFD_over_expected_mean = mean(vals),
      BFD_over_expected_sd = sd(vals),
      BFD_over_expected_lower = quantile(vals, 0.025),
      BFD_over_expected_upper = quantile(vals, 0.975)
    )
  })

  summary_list <- purrr::compact(summary_list)

  if (length(summary_list) == 0) {
    warning("No valid posterior random effects found for targeted_id, skipping Bayesian receiver rankings.")
    return(NULL)
  }

  summary_df <- dplyr::bind_rows(summary_list)

  if (is.null(summary_df) || nrow(summary_df) == 0) {
    warning("Could not locate any 'r_targeted_id' random effect columns in Bayesian draws, skipping Bayesian receiver rankings.")
    return(NULL)
  }

  summary_df |>
    arrange(desc(BFD_over_expected_mean))
}

if (sys.nframe() == 0) {
  if (file.exists("src/utils_paths.R")) {
    source("src/utils_paths.R")
  } else if (file.exists("utils_paths.R")) {
    source("utils_paths.R")
  } else {
    stop("Could not find 'utils_paths.R'.")
  }

  PROC_DIR <- get_proc_dir()

  analysis_path <- file.path(PROC_DIR, "analysis_full.rds")
  if (!file.exists(analysis_path)) {
    if (file.exists(file.path(PROC_DIR, "analysis_w01.rds"))) {
      warning("analysis_full.rds not found. Using Week 1 data.")
      analysis_path <- file.path(PROC_DIR, "analysis_w01.rds")
    } else {
      stop("No analysis data found. Run 02_compute_BFD.R first.")
    }
  }

  df <- readRDS(analysis_path)

  if (!"offense_formation" %in% names(df)) df$offense_formation <- NA_character_
  if (!"receiver_alignment" %in% names(df)) df$receiver_alignment <- NA_character_
  if (!"route_of_targeted_receiver" %in% names(df)) df$route_of_targeted_receiver <- NA_character_
  if (!"team_coverage_man_zone" %in% names(df)) df$team_coverage_man_zone <- NA_character_
  if (!"team_coverage_type" %in% names(df)) df$team_coverage_type <- NA_character_
  if (!"pass_length" %in% names(df)) df$pass_length <- NA_real_
  if (!"pass_result" %in% names(df)) df$pass_result <- NA_character_
  if (!"targeted_position" %in% names(df)) df$targeted_position <- NA_character_

  df <- df |>
    mutate(
      offense_formation = factor(replace_na(offense_formation, "Unknown")),
      receiver_alignment = factor(replace_na(receiver_alignment, "Unknown")),
      route_of_targeted_receiver = factor(replace_na(route_of_targeted_receiver, "Unknown")),
      team_coverage_man_zone = factor(replace_na(team_coverage_man_zone, "Unknown")),
      team_coverage_type = factor(replace_na(team_coverage_type, "Unknown")),
      targeted_position = replace_na(targeted_position, "Unknown")
    )

  df_filtered <- df |>
    filter(
      !is.na(pass_length),
      pass_length > 0,
      targeted_position %in% c("WR", "TE")
    )

  missing_stats <- df_filtered |>
    summarise(
      missing_BFD = sum(is.na(BFD)),
      missing_air_time = sum(is.na(air_time)),
      missing_d_throw = sum(is.na(d_throw)),
      missing_d_catch = sum(is.na(d_catch)),
      missing_targeted_id = sum(is.na(targeted_id)),
      zero_air_time = sum(air_time == 0, na.rm = TRUE),
      negative_air_time = sum(air_time < 0, na.rm = TRUE)
    )
  print(missing_stats)

  df_model <- df_filtered |>
    drop_na(BFD, air_time, d_throw, targeted_id, pass_length, defenders_in_the_box) |>
    filter(air_time >= 0) |> 
    mutate(
      targeted_id = as.factor(targeted_id),
      game_id = as.factor(game_id),
      targeted_position = factor(targeted_position),
      air_time_scaled = scale(air_time)[, 1],
      d_throw_scaled = scale(d_throw)[, 1],
      pass_length_scaled = scale(pass_length)[, 1],
      defenders_in_the_box_scaled = scale(defenders_in_the_box)[, 1]
    )

  # Fit
  mem_model <- fit_mixed_effects_model(df_model)

  if (lme4::isSingular(mem_model)) {
    message("Warning: mixed-effects model fit is singular.")
  }

  print(summary(mem_model))

  #Model validation metrics
  y_obs <- df_model$BFD
  y_hat <- fitted(mem_model)
  resid <- y_obs - y_hat

  rmse <- sqrt(mean(resid^2, na.rm = TRUE))
  mae <- mean(abs(resid), na.rm = TRUE)
  r2 <- 1 - var(resid, na.rm = TRUE) / var(y_obs, na.rm = TRUE)

  mem_metrics <- tibble(
    metric = c("RMSE", "MAE", "Pseudo_R2"),
    value  = c(rmse, mae, r2)
  )
  print(mem_metrics)

  diag_df <- tibble(
    BFD = y_obs,
    fitted = y_hat,
    resid = resid
  )

  FIG_DIR <- get_fig_dir()

  p_fit <- ggplot(diag_df, aes(x = fitted, y = BFD)) +
    geom_point(alpha = 0.3, color = "#0072B2") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
    theme_minimal() +
    labs(
      title = "Mixed-Effects Model: Fitted vs Observed BFD",
      x = "Fitted BFD",
      y = "Observed BFD"
    )

  ggsave(
    filename = file.path(FIG_DIR, "mixed_effects_fitted_vs_observed.png"),
    plot = p_fit,
    width = 8, height = 5, dpi = 300
  )

  p_resid <- ggplot(diag_df, aes(x = resid)) +
    geom_histogram(bins = 40, fill = "#D55E00", color = "white", alpha = 0.8) +
    theme_minimal() +
    labs(
      title = "Mixed-Effects Model Residuals (BFD)",
      x = "Residual",
      y = "Count"
    )

  ggsave(
    filename = file.path(FIG_DIR, "mixed_effects_residuals_hist.png"),
    plot = p_resid,
    width = 8, height = 5, dpi = 300
  )

  rankings <- extract_receiver_rankings(mem_model)

  target_counts <- df_model |>
    group_by(targeted_id) |>
    summarise(
      n_targets = n(),
      .groups = "drop"
    ) |>
    mutate(targeted_id = as.character(targeted_id))

  meta_map <- df_filtered |>
    distinct(targeted_id, targeted_name, targeted_position) |>
    mutate(targeted_id = as.character(targeted_id))

  rankings_full <- rankings |>
    left_join(meta_map, by = c("nfl_id" = "targeted_id")) |>
    left_join(target_counts, by = c("nfl_id" = "targeted_id")) |>
    relocate(targeted_name, .before = nfl_id) |>
    relocate(targeted_position, .after = targeted_name)

  rankings_qualified <- rankings_full |>
    filter(n_targets >= 30) |>
    arrange(desc(BFD_over_expected))

  print(head(rankings_qualified, 10))

  bayes_model <- fit_bayesian_BFD_model(df_model)
  bayes_rankings <- extract_bayesian_receiver_rankings(bayes_model)

  if (!is.null(bayes_model) && !is.null(bayes_rankings)) {
    bayes_rankings_full <- bayes_rankings |>
      left_join(meta_map, by = c("nfl_id" = "targeted_id")) |>
      left_join(target_counts, by = c("nfl_id" = "targeted_id")) |>
      relocate(targeted_name, .before = nfl_id) |>
      relocate(targeted_position, .after = targeted_name)

    bayes_rankings_qualified <- bayes_rankings_full |>
      filter(n_targets >= 30) |>
      arrange(desc(BFD_over_expected_mean))

    print(head(bayes_rankings_qualified, 10))

    saveRDS(bayes_model, file.path(PROC_DIR, "mixed_effects_model_bayes.rds"))
    write_csv(bayes_rankings_qualified, file.path(PROC_DIR, "receiver_rankings_bayes.csv"))
  } else {
    message("Bayesian BFD model not fit, skipping Bayesian rankings export.")
  }

  saveRDS(mem_model, file.path(PROC_DIR, "mixed_effects_model.rds"))
  write_csv(rankings_qualified, file.path(PROC_DIR, "receiver_rankings.csv"))
  write_csv(mem_metrics, file.path(PROC_DIR, "mixed_effects_metrics.csv"))

}
```

### Predictive Modeling of Final Separation (XGBoost)


While the mixed‑effects model attributes skill to receivers, we also ask: given what we know at the throw, how much separation should we expect at the catch? To answer this, we fit a gradient‑boosted tree model (XGBoost) to predict $d_{catch}$ using only pre‑throw features, including initial differential, air time, play context variables, and route indicators.

We trained the model on 80% of the randomly sampled plays in our dataset and reserved the remaining 20% as a test set to evaluate predictive performance. We use Root Mean Squared Error in yards to evaluate the performance.

```{r}
library(tidyverse)
set.seed(123)
if (!requireNamespace("xgboost", quietly = TRUE) ||
  !requireNamespace("caret", quietly = TRUE)) {
  stop("Packages 'xgboost' and 'caret' are required for 07_ml_models.R. Please install them.")
}

prepare_ml_data <- function(analysis_df) {

  desired_cols <- c(
    "d_catch", 
    "d_throw",
    "air_time",
    "pass_length",
    "defenders_in_the_box",
    "expected_points",
    "expected_points_added",
    "yards_gained",
    "offense_formation",
    "receiver_alignment",
    "route_of_targeted_receiver",
    "team_coverage_man_zone",
    "team_coverage_type",
    "pass_location_type",
    "dropback_type"
  )

  available <- intersect(desired_cols, names(analysis_df))

  ml_df <- analysis_df |>
    select(all_of(available)) |>
    filter(!is.na(d_catch), !is.na(d_throw), !is.na(air_time)) |>
    mutate(
      across(
        where(is.character),
        ~ factor(replace_na(.x, "Unknown"))
      )
    )

  ml_df
}

train_xgboost <- function(ml_df) {
  train_index <- caret::createDataPartition(ml_df$d_catch, p = 0.8, list = FALSE)
  train_data <- ml_df[train_index, ]
  test_data <- ml_df[-train_index, ]

  train_matrix <- model.matrix(d_catch ~ . - 1, data = train_data)
  test_matrix <- model.matrix(d_catch ~ . - 1, data = test_data)

  dtrain <- xgboost::xgb.DMatrix(
    data = train_matrix,
    label = train_data$d_catch
  )

  dtest <- xgboost::xgb.DMatrix(
    data = test_matrix,
    label = test_data$d_catch
  )

  params <- list(
    objective = "reg:squarederror",
    eta = 0.1,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 0.8
  )

  model <- xgboost::xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,
    watchlist = list(train = dtrain, test = dtest),
    print_every_n = 10,
    early_stopping_rounds = 10
  )

  importance <- xgboost::xgb.importance(model = model)
  importance_plot <- xgboost::xgb.plot.importance(importance_matrix = importance)

  pred_train <- predict(model, dtrain)
  pred_test <- predict(model, dtest)

  obs_train <- train_data$d_catch
  obs_test <- test_data$d_catch

  rmse_train <- sqrt(mean((obs_train - pred_train)^2, na.rm = TRUE))
  rmse_test <- sqrt(mean((obs_test - pred_test)^2, na.rm = TRUE))

  mae_train <- mean(abs(obs_train - pred_train), na.rm = TRUE)
  mae_test <- mean(abs(obs_test - pred_test), na.rm = TRUE)

  r2_train <- 1 - var(obs_train - pred_train, na.rm = TRUE) / var(obs_train, na.rm = TRUE)
  r2_test <- 1 - var(obs_test - pred_test, na.rm = TRUE) / var(obs_test, na.rm = TRUE)

  metrics <- tibble(
    split  = c("train", "test"),
    RMSE   = c(rmse_train, rmse_test),
    MAE    = c(mae_train, mae_test),
    R2     = c(r2_train, r2_test)
  )
  print(metrics)

  list(
    model = model,
    test_data = test_data,
    importance = importance,
    importance_plot = importance_plot,
    metrics = metrics,
    test_diag = tibble(
      obs  = obs_test,
      pred = pred_test
    )
  )
}


if (sys.nframe() == 0) {
  if (dir.exists("processed")) {
    PROC_DIR <- "processed"
  } else if (dir.exists("../processed")) {
    PROC_DIR <- "../processed"
  } else {
    stop("Could not find 'processed' directory.")
  }

  analysis_path <- file.path(PROC_DIR, "analysis_full.rds")
  if (!file.exists(analysis_path)) stop("analysis_full.rds not found.")

  df <- readRDS(analysis_path)
  ml_data <- prepare_ml_data(df)

  res <- train_xgboost(ml_data)

  if (!dir.exists("figures")) dir.create("figures")
  if (inherits(res$importance_plot, "ggplot")) {
    importance_fig <- res$importance_plot + ggplot2::theme_minimal(base_size = 14)
    ggplot2::ggsave(
      filename = file.path("figures", "feature_importance.png"),
      plot = importance_fig,
      width = 8,
      height = 5,
      dpi = 300
    )
  } else {
    message("Feature importance plot not available, skipping feature_importance.png.")
  }

  p_pred <- ggplot(res$test_diag, aes(x = pred, y = obs)) +
    geom_point(alpha = 0.3, color = "#009E73") +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
    theme_minimal() +
    labs(
      title = "XGBoost: Predicted vs Observed d_catch (Test Set)",
      x = "Predicted d_catch",
      y = "Observed d_catch"
    )

  ggplot2::ggsave(
    filename = file.path("figures", "xgboost_pred_vs_observed.png"),
    plot = p_pred,
    width = 8,
    height = 5,
    dpi = 300
  )

  saveRDS(res$model, file.path(PROC_DIR, "xgboost_model.rds"))
  write_csv(res$metrics, file.path(PROC_DIR, "xgboost_metrics.csv"))
}
```


```{r}
get_proc_dir <- function() {
    if (dir.exists("processed")) {
        return("processed")
    }
    if (dir.exists("../processed")) {
        return("../processed")
    }
    stop("Could not find 'processed' directory.")
}

get_fig_dir <- function() {
    if (dir.exists("../figures")) {
        return("../figures")
    }
    if (dir.exists("figures")) {
        return("figures")
    }
    dir.create("figures", showWarnings = FALSE)
    "figures"
}
```

## Results

### Separation Dynamics (BFD)

```{r}
plot_BFD_distribution <- function(data) {
  ggplot(data, aes(x = BFD)) +
    geom_histogram(binwidth = 0.5, fill = "blue", color = "white", alpha = 0.8) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "red", linewidth = 1) +
    theme_minimal() +
    labs(
      title = "Distribution: Ball-Flight Differential added (BFD)",
      x = "BFD (Yards)",
      y = "Number of Plays",
      caption = "Positive BFD = Separation Gained, Negative BFD = Separation Lost"
    )
}

p1 <- plot_BFD_distribution(df)
ggsave(file.path(FIG_DIR, "BFD_distribution.png"), p1, width = 8, height = 5)
```

![Distribution of BFD](../figures/BFD_distribution.png)
Across the 14,108 passing plays we studied, receivers generally lose separation once the ball is in the air, meaning defenders begin to close space during ball flight. This produces an average Ball-Flight Differential (BFD) of –0.82 yards, with the distribution centered slightly below zero and a long left tail. Most plays cluster around 0, where the defender and receiver maintain roughly the same cushion while the ball is traveling. The long left tail reflects the plays with deeper routes, where defenders close a large amount of space. The smaller right tail represents plays where receivers gain separation in the air through speed, leverage, route, etc.

From a football perspective, this negative baseline matters because if most plays involve separation loss, then simply maintaining separation and even slightly increasing separation is a sign of receiver skill. This also shows why looking only at separation at the catch can be misleading; some receivers routinely fight back to neutral or positive BFD despite starting in challenging positions.

### Drivers of Ball-Flight Differential 

The mixed effects model shows how initial differential, air time, and context are all relevant to contributing to the BFD. 

```{r}
plot_BFD_vs_airtime <- function(data) {
  ggplot(data, aes(x = air_time, y = BFD)) +
    geom_point(alpha = 0.5, color = "orange") +
    geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
    theme_minimal() +
    labs(
      title = "BFD vs. Air Time",
      x = "Air Time (seconds)",
      y = "BFD (Yards)"
    )
}

p2 <- plot_BFD_vs_airtime(df)
ggsave(file.path(FIG_DIR, "BFD_vs_airtime.png"), p2, width = 8, height = 5)
```

![BFD vs Air Time](../figures/BFD_vs_airtime.png)

Air Time shows that the longer the throw, the more separation is lost. The coefficient is approximately -0.09 and statistically significant with a p-value < .01. This means that adding half a second to a throw flight time costs about .045 yards of separation on average. Thus, the deeper balls give defenders more time to recover.

```{r}
plot_BFD_vs_throw_separation <- function(data) {
  ggplot(data, aes(x = d_throw, y = BFD)) +
    geom_point(alpha = 0.5, color = "aquamarine3") +
    geom_smooth(method = "lm", se = FALSE, color = "black", linetype = "dashed") +
    theme_minimal() +
    labs(
      title = "BFD vs. Initial Differential at Throw",
      x = "Throw Separation (Yards)",
      y = "BFD (Yards)"
    )
}

p3 <- plot_BFD_vs_throw_separation(df)
ggsave(file.path(FIG_DIR, "BFD_vs_dthrow.png"), p3, width = 8, height = 5)
```

![BFD vs Initial Differential](../figures/BFD_vs_dthrow.png)

Initial Differential shows that extreme situations move back toward normal by the time the ball arrives, as the coefficient is -0.39 which is statistically significant with a p-value < .01. For every extra yard of openness when the ball is thrown, the receiver loses about 0.39 yards of advantage by the time the ball throw is completed. Routes open at the throw and extreme situations tend to move back toward “normal” by the time the ball arrives. 

```{r}
library(ggplot2)

p_fit <- ggplot(diag_df, aes(x = fitted, y = BFD)) +
  geom_point(alpha = 0.3, color = "cornflowerblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(
    title = "Mixed-Effects Model of Fitted vs Observed BFD",
    x = "Fitted BFD",
    y = "Observed BFD"
  )

ggsave(
  filename = file.path(FIG_DIR, "mixed_effects_fitted_vs_observed.png"),
  plot = p_fit,
  width = 8, height = 5, dpi = 300)

p_resid <- ggplot(diag_df, aes(x = resid)) +
  geom_histogram(bins = 40, fill = "chocolate2", color = "white", alpha = 0.8) +
  theme_minimal() +
  labs(
    title = "Mixed-Effects Model Residuals (BFD)",
    x = "Residual",
    y = "Count"
  )

ggsave(
  filename = file.path(FIG_DIR, "mixed_effects_residuals_hist.png"),
  plot = p_resid,
  width = 8, height = 5, dpi = 300)
```

![Mixed-Effects Fitted vs Observed](../figures/mixed_effects_fitted_vs_observed.png)

The fitted vs observed are about centered and symmetric for a normal distribution. This can be further strengthened by the histogram below.

![Mixed-Effects Residuals](../figures/mixed_effects_residuals_hist.png)

The air time, initial differential, and covariates explain a meaningful part of the variation in the Ball-Flight Differential. Still, the variation can be accounted for by other relevant factors like the receiver, quarterback, ball location, etc.


### Markov Chain 

```{r}
plot_transition_matrix <- function(trans_probs) {
  ggplot(trans_probs, aes(x = next_state, y = fct_rev(state), fill = prob)) +
    geom_tile() +
    geom_text(aes(label = round(prob, 2)), color = "white") +
    scale_fill_gradient(low = "blue4", high = "red") +
    theme_minimal() +
    labs(
      title = "Transition Probabilities between Separation States",
      x = "Next State",
      y = "Current State",
      fill = "Probability"
    )
}

p <- plot_transition_matrix(tm)
FIG_DIR <- get_fig_dir()
ggsave(file.path(FIG_DIR, "markov_transitions.png"), p, width = 8, height = 6)
```

![Markov Transition Matrix](../figures/markov_transitions.png)

**Top Receivers by Markov Lift**:

```{r}
library(tidyverse)
library(knitr)

lift_path <- "../processed/markov_lift_rankings.csv"
lift_bayes_path <- "../processed/markov_lift_rankings_bayes.csv"

if (file.exists(lift_path)) {
  lift <- read_csv(lift_path)

  top_lift <- lift |>
    head(5) |>
    mutate(
      player_rate = paste0(round(player_rate * 100, 1), "%"),
      markov_lift = paste0("+", round(markov_lift * 100, 1), "%")
    )

  if ("targeted_name" %in% names(top_lift)) {
    top_lift |>
      select(targeted_id, targeted_name, player_rate, markov_lift) |>
      kable(col.names = c("ID", "Player", "Success Rate", "Markov Lift"))
  } else if ("displayName" %in% names(top_lift)) {
    top_lift |>
      select(targeted_id, displayName, player_rate, markov_lift) |>
      kable(col.names = c("ID", "Player", "Success Rate", "Markov Lift"))
  } else {
    top_lift |>
      select(targeted_id, player_rate, markov_lift) |>
      kable(col.names = c("ID", "Success Rate", "Markov Lift"))
  }
}
```

```{r}

if (file.exists(lift_bayes_path)) {
  lift_bayes <- read_csv(lift_bayes_path) |>
    rename(
      player_rate_lower = `player_rate_lower.2.5%`,
      player_rate_upper = `player_rate_upper.97.5%`
    )

  top_lift_bayes <- lift_bayes |>
    head(5) |>
    mutate(
      player_rate = paste0(round(player_rate_mean * 100, 1), "%"),
      cr_interval = paste0(
        "[", round(player_rate_lower * 100, 1), "%, ",
        round(player_rate_upper * 100, 1), "%]"
      )
    )

  if ("targeted_name" %in% names(top_lift_bayes)) {
    top_lift_bayes |>
      select(targeted_id, targeted_name, player_rate, cr_interval) |>
      kable(col.names = c("ID", "Player", "Break-Open Rate", "95% CrI"))
  } else if ("displayName" %in% names(top_lift_bayes)) {
    top_lift_bayes |>
      select(targeted_id, displayName, player_rate, cr_interval) |>
      kable(col.names = c("ID", "Player", "Break-Open Rate", "95% CrI"))
  } else {
    top_lift_bayes |>
      select(targeted_id, player_rate, cr_interval) |>
      kable(col.names = c("ID", "Break-Open Rate", "95% CrI"))
  }
}
```

This matrix shows how various states remain similar when moving onto the next frame, like S3 (Wide Open) versus S2 (Open). 

We use this to find receivers who are more skilled at breaking open during mid-route. The Markov Lift shows the difference between the league average and the player’s break-open probability. Thus, we can interpret big, positive Markov Lift numbers as the ability to make tight coverage into open windows more often than we would expect across many games. 

The receivers at the top of this table show that they get open more often than NFL players usually do on average. A 5% markov lift signifies a player who finds space 5% more on average than the league average. Thus, this player circumvents the defender and wins leverage, which helps them show up open on the quarterback's perspective more than others. Even after accounting for randomness, limited snaps, route variation, etc., the receivers with high lifts are still above average. With +0.19 percentage points above the league baseline average (1.49% per frame), Olave breaks open slightly more often than an average receiver in similar situations.

Thus, receivers who run a lot of routes and typically create space have smaller intervals, so we are more confident in their high openness average. Alternatively, the receivers with fewer routes or fewer roles can look fantastic, but their intervals are wide, which means we have more uncertainty about their performance being above average.

```{r}
plot_bayesian_markov_intervals <- function(lift_bayes, top_n = 15) {
  if (!"player_rate_mean" %in% names(lift_bayes)) {
    return(NULL)
  }

  if ("player_rate_lower.2.5%" %in% names(lift_bayes) &&
    !"player_rate_lower" %in% names(lift_bayes)) {
    lift_bayes <- lift_bayes |>
      dplyr::rename(
        player_rate_lower = `player_rate_lower.2.5%`,
        player_rate_upper = `player_rate_upper.97.5%`
      )
  }

  if ("targeted_name" %in% names(lift_bayes)) {
    lift_bayes <- lift_bayes |>
      mutate(label = targeted_name)
  } else {
    lift_bayes <- lift_bayes |>
      mutate(label = as.character(targeted_id))
  }

  lift_bayes |>
    filter(!is.na(player_rate_mean)) |>
    slice_max(player_rate_mean, n = top_n, with_ties = FALSE) |>
    mutate(
      label = fct_reorder(label, player_rate_mean),
      player_rate_pct = player_rate_mean
    ) |>
    ggplot(aes(x = label, y = player_rate_pct)) +
    geom_point(color = "chocolate2") +
    geom_errorbar(
      aes(
        ymin = player_rate_lower,
        ymax = player_rate_upper
      ),
      width = 0
    ) +
    coord_flip() +
    theme_minimal() +
    labs(
      title = "Top Receivers by Break-Open Rate (Bayesian Markov Lift)",
      x = "Receiver",
      y = "Break-Open Probability (%)"
    )
}

bayes_markov_path <- file.path(PROC_DIR, "markov_lift_rankings_bayes.csv")
if (file.exists(bayes_markov_path)) {
  lift_bayes <- read_csv(bayes_markov_path, show_col_types = FALSE)
  p7 <- plot_bayesian_markov_intervals(lift_bayes, top_n = 15)
  if (!is.null(p7)) {
    ggsave(file.path(FIG_DIR, "markov_lift_bayes_intervals.png"), p7, width = 8, height = 6)
  }
}
```

![Bayesian Markov Lift Intervals](../figures/markov_lift_bayes_intervals.png)


### XGBoost: Predictive Model

Our XGBoost model predicts final space differential between receiver and defender using the information provided prior to the ball being in flight. 

On the held‑out test set, the model attains an RMSE of **1.79 yards**, meaning that predictions are typically within about two yards of the observed separation. This predictive metric helps:

- Distinguish between receivers who make themselves wide‑open versus tightly positioned receivers.
- Find circumstances which typically have open throws, like routes, coverage, etc.

Overall, the model's scores to determine the signficance of various features shows that Initial Differential is most predictive, then Air Time follows suit. While there are other features that help explain and predict the results, they are not as impactful to the model. Below, we can see the predicted versus observed plot to show how the points have moderate variance:

```{r}
p_pred <- ggplot(res$test_diag, aes(x = pred, y = obs)) +
  geom_point(alpha = 0.3, color = "darkolivegreen4") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(
    title = "XGBoost: Predicted vs Observed d_catch (Test Set)",
    x = "Predicted d_catch",
    y = "Observed d_catch"
  )

ggplot2::ggsave(
  filename = file.path("figures", "xgboost_pred_vs_observed.png"),
  plot = p_pred,
  width = 8,
  height = 5,
  dpi = 300
)
```

![XGBoost Predicted vs Observed](../figures/xgboost_pred_vs_observed.png)

### Top Receivers (Ball-Flight Differential Over Expected)

```{r}
rankings_path <- "../processed/receiver_rankings.csv"
if(file.exists(rankings_path)) {
  rankings <- read_csv(rankings_path)

  top_rank <- rankings |>
    head(10)

  if ("targeted_name" %in% names(top_rank) && "targeted_position" %in% names(top_rank)) {
    top_rank |>
      select(nfl_id, targeted_name, targeted_position, BFD_over_expected) |>
      kable(col.names = c("ID", "Player", "Pos", "BFD Over Expected"), digits = 3)
  } else if ("displayName" %in% names(top_rank) && "targeted_position" %in% names(top_rank)) {
    top_rank |>
      select(nfl_id, displayName, targeted_position, BFD_over_expected) |>
      kable(col.names = c("ID", "Player", "Pos", "BFD Over Expected"), digits = 3)
  } else if ("targeted_name" %in% names(top_rank)) {
    top_rank |>
      select(nfl_id, targeted_name, BFD_over_expected) |>
      kable(col.names = c("ID", "Player", "BFD Over Expected"), digits = 3)
  } else if ("displayName" %in% names(top_rank)) {
    top_rank |>
      select(nfl_id, displayName, BFD_over_expected) |>
      kable(col.names = c("ID", "Player", "BFD Over Expected"), digits = 3)
  } else {
    top_rank |>
      select(nfl_id, BFD_over_expected) |>
      kable(col.names = c("ID", "BFD Over Expected"), digits = 3)
  }
}
```

```{r}
rankings_bayes_path <- "../processed/receiver_rankings_bayes.csv"
if (file.exists(rankings_bayes_path)) {
  rankings_bayes <- read_csv(rankings_bayes_path)

  top_rank_bayes <- rankings_bayes |>
    head(10) |>
    mutate(
      BFD_mean = round(BFD_over_expected_mean, 3),
      BFD_cri = paste0(
        "[", round(BFD_over_expected_lower, 3), ", ",
        round(BFD_over_expected_upper, 3), "]"
      )
    )

  if ("targeted_name" %in% names(top_rank_bayes)) {
    top_rank_bayes |>
      select(nfl_id, targeted_name, BFD_mean, BFD_cri) |>
      kable(col.names = c("ID", "Player", "BFD OE (Mean)", "95% CrI"))
  } else if ("displayName" %in% names(top_rank_bayes)) {
    top_rank_bayes |>
      select(nfl_id, displayName, BFD_mean, BFD_cri) |>
      kable(col.names = c("ID", "Player", "BFD OE (Mean)", "95% CrI"))
  } else {
    top_rank_bayes |>
      select(nfl_id, BFD_mean, BFD_cri) |>
      kable(col.names = c("ID", "BFD OE (Mean)", "95% CrI"))
  }
}
```
These leaderboards are restricted to wide receivers and tight ends with at least 30 forward‑pass targets in the regular season, so we have enough data to use for the receiver rankings. 

The wider confidence intervals (high variance) around the receiver’s values show that the players are not consistent, as they are open for some plays and disappear on others. On the other hand, the narrower intervals show players who are consistently reliable to gain separation in the air. 

The receivers with positive BFD Over Expected values mean that they consistently beat the model; after accounting for the throw difficulty with air time, initial differential, etc., they still end up more open at the catch than other similar players in similar situations by adjusting to the ball and having great tracking skills. Alternatively, receivers with negative values rely on the timing and scheme, while somewhat struggling to maintain leverage when the ball is released. They may be open at first, but do not sustain this openness throughout the ball's flight. 

For example, Chris Olave has an BFD Over Expected of about 0.15, with a narrower CI of -0.07 to 0.38. This means Olave is somewhat consistent by adding about .15 of a yard of separation during the throw (after already accounting for route, defender, air in time, etc.). The narrower CI says that this separation is not due to noise of lucky plays, but rather that Olave is consistent. 

On the other hand, Kalif Raymond has an BFD Over Expected of about 0.24 and a wider CI of -0.03 to 0.58. While Raymond adds about a quarter of a yard of separation during the throw (better BFD than Olave), we are not very confident that this is how he consistently plays due to his wide CI. Thus, his BFD may vary more due to scheme, play, routes, mismatches of players, etc. There may be more instances of variation when Raymond separates, versus times when he is not open at all. Overall, Olave shows a smaller, but more reliable pattern of separation to frequently breaking open.

```{r}
plot_bayesian_BFD_intervals <- function(rankings_bayes, top_n = 15) {
  if (!"BFD_over_expected_mean" %in% names(rankings_bayes)) {
    return(NULL)
  }

  if ("targeted_name" %in% names(rankings_bayes)) {
    rankings_bayes <- rankings_bayes |>
      mutate(label = targeted_name)
  } else {
    rankings_bayes <- rankings_bayes |>
      mutate(label = as.character(nfl_id))
  }

  rankings_bayes |>
    filter(!is.na(BFD_over_expected_mean)) |>
    slice_max(BFD_over_expected_mean, n = top_n, with_ties = FALSE) |>
    mutate(label = fct_reorder(label, BFD_over_expected_mean)) |>
    ggplot(aes(x = label, y = BFD_over_expected_mean)) +
    geom_point(color = "cornflowerblue") +
    geom_errorbar(
      aes(ymin = BFD_over_expected_lower, ymax = BFD_over_expected_upper),
      width = 0
    ) +
    coord_flip() +
    theme_minimal() +
    labs(
      title = "Top Receivers by BFD Over Expected (Bayesian)",
      x = "Receiver",
      y = "BFD Over Expected (Yards)"
    )
}

bayes_BFD_path <- file.path(PROC_DIR, "receiver_rankings_bayes.csv")
if (file.exists(bayes_BFD_path)) {
  rankings_bayes <- read_csv(bayes_BFD_path, show_col_types = FALSE)
  p6 <- plot_bayesian_BFD_intervals(rankings_bayes, top_n = 15)
  if (!is.null(p6)) {
    ggsave(file.path(FIG_DIR, "receiver_BFD_bayes_intervals.png"), p6, width = 8, height = 6)
  }
}
```

![Bayesian BFD Over Expected Intervals](../figures/receiver_BFD_bayes_intervals.png)


## Improvements

To improve our BFD metric, we would want to **filter by C in the pass_result** column, so we could see the effect on BFD dependent on if the throw was complete, incomplete, or intercepted. For our model we created, we wanted to avoid selection bias by not only filtering towards good plays, so we included all (complete/incomplete). Overall, we not only want to see if they have sticky hands, but how typical it is for the receiver to distance themselves from the defenders despite the outcome. We could also include the **routes with fixed effects (dig, go, etc.)** and create interaction terms to know which routes relate to more or less BFD skill. Another improvement could be to **weight the BFD metrics by the value** of their skill in the field; specifically, the catch probability and expected points added could help weigh how much the BFD affects the value on the field. 


## References

After the Full-Time Whistle. (2023). Using Markov models to analyse defensive strategies in modern football (simplified & explained). Medium. https://medium.com/after-the-full-time-whistle/using-markov-models-to-analyse-defensive-strategies-in-modern-football-simplified-explained-68097ac008e8 

Blanc, D., Luxenberg, A., & Xie, J. (2016). NFL score difference prediction with Markov modeling (CS229 Final Project Report). Stanford University. https://cs229.stanford.edu/proj2016/report/BlancLuxenbergXie-NFLScoreDifferencePredictionWithMarkovModeling-report.pdf 

Dahl, J. (n.d.). Football analytics: Modeling receiver performance and separation. Columbia University. https://www.columbia.edu/~jad2295/Projects/Football%20Analytics.pdf 

Pak, I. (n.d.). Mixing time and Markov chains. University of California, Los Angeles. https://www.math.ucla.edu/~pak/papers/stoc2.pdf 

Sinclair, A. (n.d.). Liftings for Markov chains. University of California, Berkeley. https://people.eecs.berkeley.edu/~sinclair/liftings.pdf 
